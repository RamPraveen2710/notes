import pandas as pd
import numpy as np
import ast
from datetime import datetime, timedelta

# Load the processed interaction & readership data
interaction_file = "processed_interactions.csv"
readership_file = "processed_readership.csv"

interactions_df = pd.read_csv(interaction_file, dtype=str)
readership_df = pd.read_csv(readership_file, dtype=str)

# Convert lists stored as strings back to lists
list_columns = ["FirmwideIDs", "ParticipantNames", "EmployeeInd", "Roles", "RICs", "TickersDiscussed", "EMCM_Tickers"]
for col in list_columns:
    interactions_df[col] = interactions_df[col].apply(lambda x: ast.literal_eval(x) if pd.notna(x) and x.startswith("[") else [])

# Convert date columns to datetime
interactions_df["StartTime"] = pd.to_datetime(interactions_df["StartTime"], errors="coerce")
readership_df["StartDate"] = pd.to_datetime(readership_df["StartDate"], errors="coerce")

# ✅ Step 2: Identify Interactions with the Searched Ticker & Check 30-Day Readership Window
def get_ticker_matched_interactions(ticker, interactions_df, readership_df):
    """
    Find interactions where an external participant is present and the TickerDiscussed, RIC, or EMCM_Ticker 
    contains the searched ticker. Then, check if any participant had a readership event within 30 days.
    """
    matched_interactions = set()
    external_participants = set()
    interaction_scores = {}

    # Filter readership data for the ticker
    readership_filtered = readership_df[readership_df["TickerRead"].str.contains(ticker, case=False, na=False)]

    for _, row in interactions_df.iterrows():
        # Check if the interaction contains the searched ticker in RICs, TickerDiscussed, or EMCM_Tickers
        if any(ticker.lower() in str(value).lower() for value in row["TickersDiscussed"] + row["RICs"] + row["EMCM_Tickers"]):
            # Ensure at least one external participant exists
            if any(emp == "0" for emp in row["EmployeeInd"]):  
                matched_interactions.add(row["InteractionID"])
                external_participants.update(row["FirmwideIDs"])  

                # Check if any participant has a readership event within 30 days
                for participant in row["FirmwideIDs"]:
                    if participant in set(readership_filtered["FirmwideID"]):
                        last_read_date = readership_filtered.loc[readership_filtered["FirmwideID"] == participant, "StartDate"].max()
                        if pd.notna(last_read_date) and abs((row["StartTime"] - last_read_date).days) <= 30:
                            interaction_scores[row["InteractionID"]] = 95  # Higher Score for Recent Readership
                            break  # One match is enough
                else:
                    interaction_scores[row["InteractionID"]] = 80  # Lower Score for non-recent readership

    matched_df = interactions_df[interactions_df["InteractionID"].isin(matched_interactions)].copy()
    matched_df["Score"] = matched_df["InteractionID"].map(interaction_scores)  

    remaining_interactions_df = interactions_df[~interactions_df["InteractionID"].isin(matched_interactions)].copy()

    return matched_df, remaining_interactions_df, external_participants

# ✅ Step 3: Identify Covering Analyst Interactions
def get_analyst_matched_interactions(ticker, remaining_interactions_df, external_participants):
    """
    Identify interactions where:
    - The internal participant is a covering analyst for the given ticker.
    - At least one external participant is present from our identified list.
    - Assign scores based on subject relevance.
    """
    covering_analysts = {
        "NVDA": "874459",
        "TSLA": "10327",
        "MSFT": "153425",
        "AAPL": "1161226"
    }

    matched_interactions = set()
    interaction_scores = {}

    if ticker not in covering_analysts:
        print(f"No covering analyst found for {ticker}")
        return pd.DataFrame(), remaining_interactions_df

    covering_analyst_id = covering_analysts[ticker]

    for _, row in remaining_interactions_df.iterrows():
        internal_firmwide_ids = {fid for fid, emp in zip(row["FirmwideIDs"], row["EmployeeInd"]) if emp == "1"}
        external_firmwide_ids = {fid for fid, emp in zip(row["FirmwideIDs"], row["EmployeeInd"]) if emp == "0"}

        # Check if covering analyst is internal
        if covering_analyst_id in internal_firmwide_ids:
            # Ensure at least one external participant is in our identified list
            if external_firmwide_ids & external_participants:  
                matched_interactions.add(row["InteractionID"])

                # Assign scores based on subject match
                subject_text = str(row["Subject"]).lower()
                if "nvda" in subject_text or "nvidia" in subject_text:
                    interaction_scores[row["InteractionID"]] = 90  # Strong Context
                else:
                    interaction_scores[row["InteractionID"]] = 75  # Weaker Context

    matched_df_analyst = remaining_interactions_df[remaining_interactions_df["InteractionID"].isin(matched_interactions)].copy()
    matched_df_analyst["Score"] = matched_df_analyst["InteractionID"].map(interaction_scores)

    remaining_interactions_df = remaining_interactions_df[~remaining_interactions_df["InteractionID"].isin(matched_interactions)].copy()

    return matched_df_analyst, remaining_interactions_df

# ✅ Run All Steps
searched_ticker = "NVDA"

# Step 2 (Modified): Get initial interactions based on Ticker + 30-Day Check
matched_df_ticker, remaining_interactions_df, external_participants = get_ticker_matched_interactions(
    searched_ticker, interactions_df, readership_df
)

# Step 3: Get interactions with Covering Analyst
matched_df_analyst, remaining_interactions_df = get_analyst_matched_interactions(
    searched_ticker, remaining_interactions_df, external_participants
)

# ✅ Final Merging of Results
final_results = pd.concat([matched_df_ticker, matched_df_analyst], ignore_index=True)

# ✅ Save the results
final_results.to_csv("potential_interactions_nvda.csv", index=False)

# ✅ Print Output
print(f"Final potential interactions found for '{searched_ticker}':", final_results.shape[0])








import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Load precomputed embeddings
interaction_embeddings = np.load("interaction_subject_embeddings.npy")  # Shape (num_interactions, embedding_dim)
interaction_ids = pd.read_csv("interaction_ids.csv")["InteractionID"].tolist()  # Load corresponding interaction IDs

# Function to get the embedding of a given ticker
def get_ticker_embedding(ticker):
    """Generate a simple embedding for the ticker (or use a pre-trained model if available)."""
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer("all-MiniLM-L6-v2")
    return model.encode(ticker.lower())

def get_related_interactions_using_embeddings(ticker, matched_df, remaining_interactions_df, external_participants):
    """
    Find interactions within ±15 days that contain external participants from Step 1 and
    apply *semantic search* on interaction subjects using embeddings.
    """
    matched_interactions = set(matched_df["InteractionID"])
    min_date = matched_df["StartTime"].min() - timedelta(days=15)
    max_date = matched_df["StartTime"].max() + timedelta(days=15)

    filtered_df = remaining_interactions_df[
        (remaining_interactions_df["StartTime"] >= min_date) & 
        (remaining_interactions_df["StartTime"] <= max_date)
    ]

    # Generate embedding for the query ticker
    ticker_embedding = get_ticker_embedding(ticker).reshape(1, -1)

    selected_interactions = set()
    interaction_scores = {}

    for _, row in filtered_df.iterrows():
        external_ids = {fid for fid, emp in zip(row["FirmwideIDs"], row["EmployeeInd"]) if emp == "0"}
        
        if external_ids & external_participants:  # If external participant matches
            interaction_id = row["InteractionID"]

            # Fetch corresponding embedding
            if interaction_id in interaction_ids:
                idx = interaction_ids.index(interaction_id)
                interaction_embedding = interaction_embeddings[idx].reshape(1, -1)
                
                # Compute cosine similarity
                similarity_score = cosine_similarity(ticker_embedding, interaction_embedding)[0][0] * 100  # Convert to percentage
                
                if similarity_score >= 50:  # If similarity meets threshold
                    selected_interactions.add(interaction_id)
                    interaction_scores[interaction_id] = similarity_score  # Assign score based on similarity
            
    matched_df_step2 = remaining_interactions_df[remaining_interactions_df["InteractionID"].isin(selected_interactions)].copy()
    matched_df_step2["Score"] = matched_df_step2["InteractionID"].map(interaction_scores)

    remaining_interactions_df = remaining_interactions_df[~remaining_interactions_df["InteractionID"].isin(selected_interactions)].copy()
    
    return matched_df_step2, remaining_interactions_df
