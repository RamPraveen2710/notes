# Combine relevant fields into a single text representation
df["text"] = df["subject"] + " " + df["participants"] + " " + df["tags"]

# Print the first few rows
print(df[["id", "text"]])


from sentence_transformers import SentenceTransformer

# Load pre-trained sentence transformer model
model = SentenceTransformer("all-MiniLM-L6-v2")

# Convert interaction texts into vector embeddings
embeddings = model.encode(df["text"].tolist(), convert_to_tensor=True)

# Print the shape of generated embeddings
print("Embedding Shape:", embeddings.shape)  # (num_interactions, 384)



from sklearn.metrics.pairwise import cosine_similarity
import torch

def search_interactions(query, df, embeddings, model, top_k=3):
    # Convert user query into an embedding
    query_embedding = model.encode([query], convert_to_tensor=True)

    # Compute cosine similarity between the query and all stored embeddings
    similarities = cosine_similarity(query_embedding.cpu(), embeddings.cpu())[0]

    # Get the indices of top-K most similar interactions
    top_indices = similarities.argsort()[-top_k:][::-1]

    # Retrieve the matching interactions
    results = df.iloc[top_indices].copy()
    results["score"] = similarities[top_indices]  # Add similarity score

    return results

# Example search
query = "Dinner with Google CEO"
search_results = search_interactions(query, df, embeddings, model)

# Print results
print(search_results[["id", "subject", "score"]])





from transformers import AutoModel, AutoTokenizer
import torch

# Load a different semantic search model (e.g., BERT, RoBERTa, DistilBERT)
model_name = "sentence-transformers/all-mpnet-base-v2"  # A strong alternative
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def get_embedding(text):
    tokens = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        output = model(**tokens)
    return output.last_hidden_state.mean(dim=1)  # Get sentence-level embedding

# Example usage
query_embedding = get_embedding("Meeting with Google CEO")
print(query_embedding.shape)  # Output shape: (1, 768)






pip install torch transformers scikit-learn faiss-cpu
import torch
from transformers import AutoTokenizer, AutoModel
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
# Load the tokenizer and model
model_name = "sentence-transformers/all-mpnet-base-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Function to generate embeddings
def get_embedding(text):
    tokens = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        output = model(**tokens)
    return output.last_hidden_state.mean(dim=1).squeeze().numpy()  # Mean pooling
# Convert all stored interactions into embeddings
embeddings = np.array([get_embedding(text) for text in df["text"]])

# Save embeddings for future use (avoid recomputing)
np.save("embeddings.npy", embeddings)
def search_interactions(query, df, embeddings, top_k=5):
    query_embedding = get_embedding(query).reshape(1, -1)  # Convert query to embedding
    similarities = cosine_similarity(query_embedding, embeddings)[0]  # Compute similarity scores
    top_indices = similarities.argsort()[-top_k:][::-1]  # Get top K matches

    # Retrieve matching interactions
    results = df.iloc[top_indices].copy()
    results["score"] = similarities[top_indices]  # Add similarity score

    return results
query = "Dinner with Google CEO"
search_results = search_interactions(query, df, embeddings)

# Print top matches
print(search_results[["id", "subject", "score"]])














import pandas as pd

# Load the dataset (replace 'your_file.xlsx' with actual file path)
file_path = "your_file.xlsx"
df = pd.read_excel(file_path)

# Fill NaN values with empty strings to avoid issues in concatenation
df = df.fillna("")

# Grouping function to aggregate interaction details
def aggregate_interaction(group):
    """Aggregates multiple rows per interaction ID into a single text sentence."""
    return pd.Series({
        "Subject": group["Subject"].iloc[0],  
        "Purpose": group["Purpose"].iloc[0],  
        "Tag": group["Tag"].iloc[0],  
        "Participants": " ".join(group["Participant_Name"].unique()),  
        "Emails": " ".join(group["Email_Address"].unique()),  
        "Titles": " ".join(group["Title"].unique()),  
        "Functional_Roles": " ".join(group["Functional_Role"].unique()),  
        "Party_Name": group["Party_Name"].iloc[0],  
        "Party_Site": group["Party_Site"].iloc[0],  
    })

# Apply aggregation on grouped data
aggregated_df = df.groupby("ID").apply(aggregate_interaction).reset_index()

# Create a new column "Interaction_Text" with combined context
aggregated_df["Interaction_Text"] = (
    aggregated_df["Subject"] + " " +
    aggregated_df["Purpose"] + " " +
    aggregated_df["Tag"] + " " +
    aggregated_df["Participants"] + " " +
    aggregated_df["Emails"] + " " +
    aggregated_df["Titles"] + " " +
    aggregated_df["Functional_Roles"] + " " +
    aggregated_df["Party_Name"] + " " +
    aggregated_df["Party_Site"]
)

# Save the final dataset with Interaction_Text column
aggregated_df.to_csv("processed_interactions.csv", index=False)

# Print sample output
print(aggregated_df.head())



df = df.merge(aggregated_df[['ID', 'Interaction_Text']], on='ID', how='left')



