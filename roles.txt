
Problem Statement:


Limitations of Rule based Approach :

*Rule-based approach rely on predefined weights and thresholds, which fail to adapt to new patterns and edge cases.

Why Machine learning Approach:

*Machine learning model automatically identify patterns and relationships between features based on the real data.
*They adapt to the data instead of relying on hardcoded rules, making them more robust to variations.
*ML models can weigh features dynamically, understanding how they interact.
*Machine learning generalizes well, meaning it can better handle interactions with partial matches or borderline probabilities.
*Machine learning assigns probabilities dynamically, capturing nuanced relationships and producing finer-grained predictions.



Feature Engineering:

Start time difference:

*Difference between start time of both interactions, a key factor in interaction similarity.

Overlap Matching Percentage:

*It indicates how much of one interaction’s schedule coincides with another interaction.

*Interactions with a high degree of time overlap are more likely to be duplicates, especially in scenarios where participants are shared.

*Time difference alone cannot capture the extent to which interactions coincide.

	Example: Two meetings starting 5 minutes apart may have minimal overlap (e.g., 10%) or complete overlap (e.g., 90%),
	 which significantly affects their duplication probability.

Participant Matching:

*What it measures: The proportion of participants in the new interaction that match with the participants in the old interaction.
*Why it’s needed: Helps determine how closely the new interaction resembles the old one in terms of attendees.
*Works well for detecting whether the new interaction is a continuation, repeat, or direct duplicate of the old interaction.
*Relative to the old interaction because the old interaction is treated as the reference or baseline.

Particpant Overlap:

* The ratio of matching participants relative to the union of participants across both interactions (total unique participants).


Why matching and overlap seperate:
*participant matching separately offers specific advantages and granularity that cannot be fully replaced by overlap. 
*Participant Matching measures the extent to which the old interaction is "covered" by the new interaction's attendees, using the old interaction as the reference.

Why Role matching :

* Some time the significance of duplication depends on the Role of Participant matching 
* Role matching ensures that high-weight roles are given priority when assessing similarity, instead of treating all participants equally.
* For example a meeting that matches high-weight roles (e.g., multiple board members) might indicate a strategic continuation.
	


How the target label been calcualted 

Why Use the Weighted Approach?
Ensures Consistency:
* Weighted labels ensure consistent label assignment across the dataset, based on measurable criteria.Reduces subjectivity compared to manually assigned labels.
Incorporates Feature Relationships:
*Weights reflect the relative importance of features in determining duplication probability.
Example: External matching may have more influence than time difference, which is captured in the weights.
Scales with Dataset Size:

*Manually assigning labels for large datasets is impractical and error-prone.
*Weighted calculations automate label assignment, saving time and ensuring scalability.
Provides a Generalized Baseline:

****Weighted labels serve as a baseline for training, ensuring the model learns patterns aligned with domain knowledge.


Draw Backs of Manual Labeling for data points: 

* Time-Consuming: For large datasets, manual labeling is slow and resource-intensive.

* Subjectivity: Labels are prone to human bias, leading to inconsistencies in the dataset.
 Example: One expert might assign 0.7 probability; another might assign 0.5 for the same interaction.
* Limited Feature Interaction: Manual labels may fail to account for complex relationships between features (e.g., interaction between participant overlap and subject matching).
* Scalability:Manual labeling cannot scale efficiently for datasets with millions of rows.


Why XGBoost Was Chosen

The dataset involves tabular features.XGBoost excels at capturing relationships and patterns in tabular data better than other algorithms 
XGBoost provides feature importance scores, allowing to understand how features contribute to the predictions.
The model captures complex feature interactions (e.g., time overlap combined with role matching) that are difficult to encode with simple rules.
XGBoost is less sensitive to noise and missing data, ensuring reliable predictions even in real-world messy datasets.

Difference Between Manual Probability Calculation vs. Machine Learning Prediction

Manual Probability Calculation

*Each feature contributes a specific weight toward the final probability.
*The probability is determined solely by multiplying the feature values by their respective weights and summing the results.

Deterministic: Always produces the same result for the same inputs.
No Adaptability: Cannot dynamically adjust to unseen patterns or relationships.
No Feature Interactions: Treats features as independent and does not capture interactions.


Machine Learning Prediction


As The model is trained on historical data and learns relationships between features and the target probability.

Dynamic and Adaptive: Can adjust predictions based on unseen patterns in data.
Feature Interactions: Captures non-linear and interactive effects among features.
Generalization: Adapts to diverse real-world scenarios better than manual calculations.



Example 

It also learns interactions, e.g., if internal matching is low but external matching is high, the probability increases slightly.


Model uses decision trees (in XGBoost) to navigate combinations of feature values.

It predicts a probability of 
0.82
0.82, higher than the manual calculation because it recognizes an interaction between "External Matching" and "Role Matching (External)" that the manual approach misses.



Advantages of Machine Learning
Dynamic Adjustments:
ML considers all features holistically. For instance, a high external matching and high overlap might increase the probability beyond the manual calculation.
Captures Non-Linearity:
Features like time difference may have a diminishing effect as they increase, which is hard to encode in manual calculations.
Generalization:
ML adapts better to edge cases and complex patterns that manual weights cannot anticipate.




how the machine learning model predicts the output:

If the external matching is high (e.g., 90%) and the overlap is moderate (e.g., 60%), the machine learning model can dynamically increase the predicted probability, understanding that external matching plays a stronger role in these scenarios.
Edge cases, such as interactions with extreme values or unique combinations, are poorly handled by manual weights.
Machine learning models can adjust dynamically based on training data to better predict outcomes in edge cases.
It generalizes beyond the weighted rules and makes predictions even for unseen scenarios.
Goes beyond static weights by dynamically learning and optimizing feature interactions.
Adapts to real-world complexities that a manual approach cannot capture.

