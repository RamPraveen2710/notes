import pandas as pd

# âœ… Combine all matched interactions
final_potential_interactions = pd.concat([
    matched_df_high_confidence,  # From Step 2.2 (Ticker-based filtering)
    matched_df_analyst  # From Step 3 (Covering Analyst interactions)
])

# âœ… Sort by Score in Descending Order
final_potential_interactions = final_potential_interactions.sort_values(by="Score", ascending=False)

# âœ… Select Relevant Columns for Display
columns_to_display = ["InteractionID", "StartTime", "EndTime", "Subject", "FirmwideIDs", "EmployeeInd", "Roles", "TickersDiscussed", "Score"]
final_potential_interactions = final_potential_interactions[columns_to_display]

# âœ… Display Output Clearly
print("\nðŸ”¹ Final Ranked Potential Interactions ðŸ”¹")
print(final_potential_interactions.to_string(index=False))  # Ensures all rows print neatly

# âœ… Save the Output for Future Use
final_potential_interactions.to_csv("final_potential_interactions.csv", index=False)


import pandas as pd
import numpy as np
import ast
from sentence_transformers import SentenceTransformer

# âœ… Load Processed Interaction & Readership Data
interaction_file = "processed_interactions.csv"
readership_file = "processed_readership.csv"

interactions_df = pd.read_csv(interaction_file, dtype=str)
readership_df = pd.read_csv(readership_file, dtype=str)

# âœ… Convert List Columns Stored as Strings Back to Lists
list_columns = ["FirmwideIDs", "ParticipantNames", "EmployeeInd", "Roles", "RICs", "TickersDiscussed", "EMCM_Tickers"]
for col in list_columns:
    interactions_df[col] = interactions_df[col].apply(lambda x: ast.literal_eval(x) if pd.notna(x) and x.startswith("[") else [])

# âœ… Step 1: Get Interested Readers (FirmwideIDs) from Readership Data
def get_interested_readers(ticker, readership_df):
    """Find unique FirmwideIDs from readership data for a given ticker."""
    filtered_readers = readership_df[readership_df["TickerRead"].str.contains(ticker, case=False, na=False)]
    return set(filtered_readers["FirmwideID"].unique())

# âœ… Step 2: Identify Interactions with the Searched Ticker
def get_ticker_matched_interactions(ticker, interactions_df):
    """Find interactions where an external participant is present and the TickerDiscussed, RIC, or EMCM_Ticker contains the searched ticker."""
    matched_interactions = set()
    external_participants = set()
    interaction_scores = {}

    for _, row in interactions_df.iterrows():
        if any(ticker.lower() in str(value).lower() for value in row["TickersDiscussed"] + row["RICs"] + row["EMCM_Tickers"]):
            if any(emp == "0" for emp in row["EmployeeInd"]):  # External participants have EmployeeInd = 0
                matched_interactions.add(row["InteractionID"])
                external_participants.update(fid for fid, emp in zip(row["FirmwideIDs"], row["EmployeeInd"]) if emp == "0")
                interaction_scores[row["InteractionID"]] = 100  # Highest Score for Direct Ticker Match

    matched_df_ticker = interactions_df[interactions_df["InteractionID"].isin(matched_interactions)].copy()
    matched_df_ticker["Score"] = matched_df_ticker["InteractionID"].map(interaction_scores)

    return matched_df_ticker, external_participants

# âœ… Step 3: Identify Covering Analyst Interactions
def get_analyst_matched_interactions(ticker, remaining_interactions_df, external_participants):
    """Find interactions where the internal participant is a covering analyst, and an external participant is in our identified list."""
    covering_analysts = {
        "NVDA": "874459", "TSLA": "10327", "MSFT": "153425", "AAPL": "1161226"
    }
    
    if ticker not in covering_analysts:
        print(f"No covering analyst found for {ticker}")
        return pd.DataFrame(), remaining_interactions_df

    covering_analyst_id = covering_analysts[ticker]
    matched_interactions = set()
    interaction_scores = {}

    for _, row in remaining_interactions_df.iterrows():
        internal_firmwide_ids = {fid for fid, emp in zip(row["FirmwideIDs"], row["EmployeeInd"]) if emp == "1"}
        external_firmwide_ids = {fid for fid, emp in zip(row["FirmwideIDs"], row["EmployeeInd"]) if emp == "0"}

        if covering_analyst_id in internal_firmwide_ids and external_firmwide_ids & external_participants:
            matched_interactions.add(row["InteractionID"])
            subject_text = str(row["Subject"]).lower()
            interaction_scores[row["InteractionID"]] = 90 if "nvda" in subject_text or "nvidia" in subject_text else 75

    matched_df_analyst = remaining_interactions_df[remaining_interactions_df["InteractionID"].isin(matched_interactions)].copy()
    matched_df_analyst["Score"] = matched_df_analyst["InteractionID"].map(interaction_scores)

    remaining_interactions_df = remaining_interactions_df[~remaining_interactions_df["InteractionID"].isin(matched_interactions)].copy()

    return matched_df_analyst, remaining_interactions_df

# âœ… Step 4: NLP-Based Context Matching (Lower Confidence)
def get_nlp_matched_interactions(ticker, remaining_interactions_df, external_participants):
    """Use NLP embeddings to identify interactions where the subject or notes contextually relate to the searched ticker."""
    model = SentenceTransformer("all-MiniLM-L6-v2")  # Lightweight NLP Model for embeddings
    matched_interactions = set()
    interaction_scores = {}

    ticker_embedding = model.encode(ticker.lower())  # Encode the searched ticker

    for _, row in remaining_interactions_df.iterrows():
        subject_text = str(row["Subject"]).lower()
        if row["InteractionID"] in matched_interactions:
            continue  # Skip if already matched
        
        # Encode interaction subject and compute similarity
        interaction_embedding = model.encode(subject_text)
        similarity_score = np.dot(interaction_embedding, ticker_embedding) / (np.linalg.norm(interaction_embedding) * np.linalg.norm(ticker_embedding))

        if similarity_score > 0.7:  # Threshold for NLP-based match
            matched_interactions.add(row["InteractionID"])
            interaction_scores[row["InteractionID"]] = 65  # NLP Context Score

    matched_df_nlp = remaining_interactions_df[remaining_interactions_df["InteractionID"].isin(matched_interactions)].copy()
    matched_df_nlp["Score"] = matched_df_nlp["InteractionID"].map(interaction_scores)

    remaining_interactions_df = remaining_interactions_df[~remaining_interactions_df["InteractionID"].isin(matched_interactions)].copy()

    return matched_df_nlp, remaining_interactions_df

# âœ… Step 5: Combine & Save Results
def save_final_results(*dfs, filename="final_matched_interactions.npy"):
    """Combine all matched interactions and save as .npy for future use."""
    final_df = pd.concat(dfs, ignore_index=True)
    final_df.to_csv("final_matched_interactions.csv", index=False)  # Save CSV
    np.save(filename, final_df.to_numpy())  # Save as NumPy file for efficient retrieval
    print(f"Final dataset saved: {filename}")

# âœ… Run the Pipeline for a Given Ticker
searched_ticker = "NVDA"

# Step 2: Get high-confidence matches
matched_df_ticker, external_participants = get_ticker_matched_interactions(searched_ticker, interactions_df)

# Step 3: Get analyst-matched interactions
matched_df_analyst, remaining_interactions_df = get_analyst_matched_interactions(searched_ticker, remaining_interactions_df, external_participants)

# Step 4: Get NLP-matched interactions
matched_df_nlp, remaining_interactions_df = get_nlp_matched_interactions(searched_ticker, remaining_interactions_df, external_participants)

# Step 5: Save all matched interactions
save_final_results(matched_df_ticker, matched_df_analyst, matched_df_nlp)
